\chapter{Hot electron temperature modelling}
In this chapter, we will present models of the hot electron absorption trained on the dataset discussed in the previous chapter. The models will be compared and we will also discuss how well the best model represents the behaviour of $T_{\mathrm{hot}}$ compared to other studies. We will also present a simple graphing UI tool which allows to load any supported model and plot the predictions in any axis.

We do not implement the models ourselves, but rather we use optimized open-source libraries that offer rich possibilities of configuration. We will compare the models with each other using mostly \textit{RMSE} metric and $R^2$ statistic. We do not claim that all three models are exhaustingly optimized. We also do not claim that the model we label as best is the only correct solution. The goal of this chapter is to show, how the models work in practice, what are their strengths an weaknesses in this application and what can maybe done in the future to improve them.

We found that the models perform poorly if the data is not transformed. Transforming data is a common step before training a machine learning model. We decided to scale all three parameters to a closed interval [0,1] ($L$ and $I$ logarithmically). If one would skip this step, in SVR and GP, the problem quickly arises because of the absolute value of the distance between two data points needed for the kernel function. For example, if the intensity scale is $10^{17} - 10^{19}$ and scale of angles is $0-60$, the dataset is too sparse for these models to learn the relationship because the distance in the intensity dimension is unproportionally bigger. Moreover, in the parameter space, the distance between $10^{17}$ and $10^{18}$ is roughly ten times smaller than distance between $10^{18}$ and $10^{19}$. The logarithmic transformation is a justified data manipulation based on the visual inspection and the expected nature of the modelled relationship.

In the actual implementation, the classes which represent all three models follow an abstract structure that allows us to work with different models flexibly. For example, the instance of a transformer class used in the training is saved to a member variable of the model class. In general, the parameters of the transformer can differ for different models. Also, after training using transformation the data have to be transformed in the same way before the prediction as well.

For each model, we will now provide a brief introduction regarding the implementation. Then we compare the performance of these models using 8-fold cross-validation. In the end, we also compare the models by visual inspection of the prediction graphs sampled on for the same parameters as in the presentation of the dataset.

\section{Models}


For training the SVR model we used \textit{SVR} class from Python \textit{scikit-learn} library. For kernel we selected the RBF kernel. There are multiple hyper-parameters to be optimized - $\epsilon$ and $C$ from SVR itself and $\gamma$ from the RBF kernel which is here defined as $\mathrm{RBF}(\bm{x},\bm{x^\prime}) = \mathrm{exp}(\gamma \norm{\bm{x}-\bm{x^\prime}})$. During the cross-validation, where we calculate \textit{RMSE} and $R^2$ on test set for each fold, the hyper-parameters are selected via grid-search using an additional cross-validation on the training set.

The neural network model was trained using the \textit{PyTorch} library in Python. The architecture is a fully-connected neural network with two hidden layers, each containing 64 neurons with \textit{ELU} activation functions. A dropout rate of 0.2 and a weight decay of $0.1$ were applied to compensate the over-parametrization. The model was optimized using the \textit{Adam} optimizer over 2500 epochs with a learning rate of 0.02 minimizing the \textit{MSE} loss function. These hyper-parameters were chosen based on multiple attempts to find a good NN model.

Gaussian processes regression was done using \textit{GPRegression} from \textit{GPy} library also in Python. \textit{GPy} is a very robust and numerically stable tool for Gaussian process regression and classification. We used the \textit{Mattern} kernel defined by \ref{eq:mattern-kernel} with $\nu = 3/2$. The hyper-parameters are $\sigma$ (observation noise), $\sigma_k^2$ (kernel variance) and $l$ (kernel scale). The GPy class \textit{GPy.models.GPRegression} has its own optimizer, which can find the optimal hyper-parameters using maximum likelihood approach very effectively. 

\subsection*{K-Fold crossvalidation of the models}
\begin{table}[h]
	\centering
	\begin{tabular}{ c c c c c c c }
		\toprule
		\  & $SVR_\mathrm{rmse}$& $SVR_\mathrm{r2}$ & $NN_\mathrm{rmse}$   & $NN_\mathrm{r2}$ & $GP_\mathrm{rmse}$ & $GP_\mathrm{r2}$ \\ 
		\midrule
		Fold 1 & 35.13 & 0.96 & 30.97 & 0.97 & 27.55 & 0.98 \\ 
		Fold 2 & 47.61 & 0.93 & 40.82 & 0.95 & 37.28 & 0.96 \\ 
		Fold 3 & 33.18 & 0.98 & 31.51 & 0.98 & 23.94 & 0.99 \\ 
		Fold 4 & 79.91 & 0.90 & 43.35 & 0.97 & 40.80 & 0.97 \\ 
		Fold 5 & 36.95 & 0.97 & 30.09 & 0.98 & 31.05 & 0.98 \\ 
		Fold 6 & 69.68 & 0.90 & 43.04 & 0.96 & 37.00 & 0.97 \\ 
		Fold 7 & 41.01 & 0.95 & 37.71 & 0.96 & 33.78 & 0.97 \\ 
		Fold 8 & 60.28 & 0.94 & 41.18 & 0.97 & 38.96 & 0.97 \\ 
		\midrule
		Mean & 50.47 & & 37.33 & & 33.80 & \\ 
		\bottomrule
	\end{tabular}
	\caption{Cross-validation of SVR, NN and GP models.} 
	\label{tab:cross-val} 
\end{table}

For all three selected models, we performed an 8-fold cross-validation. This approach allows for a robust and reliable comparison between the models. For each fold, we calculated the root mean square error (RMSE) and the $R^2$ statistic on the test data. The detailed results for each fold, along with the mean values, are presented in Table \ref{tab:cross-val}.

As shown in the table, the mean square errors are of the same order for all three models. Some folds tend to produce worse results in general, which is expected. Specifically, it is guaranteed that in one of the folds, the dataset's minimum or maximum value might be missing from the training set. This can result in a higher RMSE due to such an \textit{unlucky} division. This is precisely why we perform 8-fold cross-validation, as on average, these random factors cancel out. The high values of $R^2$ hint that the models are able to capture the modeled relationship at least roughly. The SVR model demonstrates the worst predictive performance. In contrast, the NN and GP models exhibit similar results, though the NN is better only in fold 5. These two models will be compared in greater detail in the following sections.


\subsection*{Neural Network model}
The predictions of NN model trained with the same technique as before but now on the whole dataset can be seen in the figure \ref{fig:nn-pred}. The parameter space is the same as in previous chapter when we were presenting the dataset. Now we increased the density by which the model is sampled to grid 50 by 50 (also logarithmic in $L$ axis).
\begin{figure}[ht]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/nn17_pred}
		\caption{Predictions of NN model for $I = 1 \times 10^{17} \, \mathrm{W.cm}^{-2}$.}
		\label{fig:nn-pred-a}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/nn18_pred}
		\caption{Predictions of NN model for $I = 1 \times 10^{18} \, \mathrm{W.cm}^{-2}$.}
		\label{fig:nn-pred-b}
	\end{subfigure}
	\begin{subfigure}{0.59\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/nn19_pred}
		\caption{Predictions of NN model for $I = 1 \times 10^{19} \, \mathrm{W.cm}^{-2}$.}
		\label{fig:nn-pred-c}
	\end{subfigure}
	\caption{Predictions of NN model.}
	\label{fig:nn-pred}
\end{figure}

With the NN model, achieving smoothness is quite challenging because the fitting process does not explicitly account for the relationship between two data points. Regularization techniques such as weight decay and dropout do provide some improvement, though likely not as much as desired. The presence of sharp lines or ridges in the graphs indicates that the model does not generalize the relationships well enough. While further regularization could potentially increase smoothness, the results seen in the prediction graphs are probably expected given the small size of the dataset.

Despite these issues, the maxima appear to be in the correct parameter regions, and the maximum hot electron temperature is reasonably consistent with the values present in the dataset. However, it is important to note that the neural network can produce negative predictions, which do not have any physical interpretation. To address this, we decided to convert all negative predictions to zero.

Generally, it is possible to claim that the over-parametrized model can be smoothened to acceptable condition. However, it is still not completely clear, whether this architecture can capture the true physical relationship or not. The ridges, which are visible for predictions with $I = 10^{11} \, \mathrm{W.cm}^{-2}$ suggest either that we need more data, or that the total number of neurons is a little low.


\subsection*{Gaussian process regression model}
\begin{figure}[ht]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/gp17_pred}
		\caption{Predictions of GP model for $I = 1 \times 10^{17} \, \mathrm{W.cm}^{-2}$.}
		\label{fig:gp-pred-a}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/gp18_pred}
		\caption{Predictions of GP model for $I = 1 \times 10^{18} \, \mathrm{W.cm}^{-2}$.}
		\label{fig:gp-pred-b}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/gp19_pred}
		\caption{Predictions of GP model for $I = 1 \times 10^{19} \, \mathrm{W.cm}^{-2}$.}
		\label{fig:gp-pred-c}
	\end{subfigure}
	\caption{Predictions of the GP model.}
	\label{fig:gp-pred}
\end{figure}

The GP regression model has shown the best performance on the cross-validation test. Let us remind, that once the optimal parameters of the kernel are found, the predictions are calculated from the posterior distribution as it was presented in section \ref{sec:gp-theory}. The kernel parameters optimized on the whole dataset are $\sigma_k^2 =29179.49$, $l = 0.36$ and $\sigma^2 = 665.06$. The predictions shown in similar way as for NN model can be seen in figure \ref{fig:gp-pred}.

Immediately one can see, that these predictions are much smoother than those of NN model although in general, they are very similar. Probably the biggest qualitative difference can be found in the predictions for $I = 1 \times 10^{19} \, \mathrm{W.cm}^{-2}$, where the maximum temperature is lower for NN, whereas in GP the peak has almost exactly the same value as the dataset. Also, in graph for $I = 1 \times 10^{18} \, \mathrm{W.cm}^{-2}$, there is a local maximum for $\alpha = 50\degree$ and $L \approx 1 \, \mu\mathrm{m}$, which cannot be observed in the predictions of NN model.

\begin{figure}[ht]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/gp17_pred_ss}
		\caption{Uncertainty of GP model for $I = 1 \times 10^{17} \, \mathrm{W.cm}^{-2}$.}
		\label{fig:gp-pred-ss-a}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/gp18_pred_ss}
		\caption{Uncertainty of GP model for $I = 1 \times 10^{18} \, \mathrm{W.cm}^{-2}$.}
		\label{fig:gp-pred-ss-b}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/gp19_pred_ss}
		\caption{Uncertainty of GP model for $I = 1 \times 10^{19} \, \mathrm{W.cm}^{-2}$.}
		\label{fig:gp-pred-ss-c}
	\end{subfigure}
	\caption{Uncertainty of the GP model.}
	\label{fig:gp-pred-ss}
\end{figure}

Now let us come back to one of the biggest strengths of the GP model. As it was stated in the section \ref{sec:gp-theory}, the GP model consists of mean function and covariance function. The predictions in \ref{fig:gp-pred} are the mean evaluated in given points. The variance of these points is calculated as diagonal elements of a matrix corresponding to the covariance function in the testing points. These values are scaled with optimized parameter of GP - noise variance $\sigma^2$. The interpretation of this parameter is therefore straightforward.

It is possible to create same plots but for the standard error $\sigma$. Such plots are shown in the figure \ref{fig:gp-pred-ss}. Naturally, the uncertainty is smaller where the training data is denser and larger in areas with sparse data. The denser the dataset, the smaller the error. The dataset was primarily created using a grid distribution of the simulation parameters. The plots hint that the dataset is denser for small angles (up to 10Â°), which is true. Additionally, there is a higher density of data for the intensity $I = 10^{17} \, \mathrm{W.cm}^{-2}$ around $L = 0.5\mu \mathrm{m}$ and for $\alpha$ in range of $15\degree$ to $ 30\degree$, as confirmed by the graphs.

\section{Prediction UI tool}
For the purpose of visual comparison of the models, another tool was developed. The main goal is to allow user to be able to look at the predictions of different models in different axes than those which we used until now. Plotting of custom looks can be tedious if one is writing each time a new script. Also, working with 4 dimensional data is unpleasant, because it is not possible to draw everything in one graph without sacrificing readability.

The tool provides an user interface for several basic options that are relevant for this analysis and a graph of the predictions based on the configuration. A screenshot from this application is shown in the figure \ref{fig:graph-tool}.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.85 \textwidth]{figures/graph_tool}
	\caption{A screenshot from the graphing tool with graph of GP model for $\alpha = 0\degree$.}
	\label{fig:graph-tool}
\end{figure}

The tool is quite simple, but it is tailored for this application, so it almost exhaustingly fulfils the need for special tool. It might be useful even beyond the scope of this thesis for example if someone decides to improve the models we presented or if he just wants to inspect them more closely. 

The application is implemented in Python in \textit{PyQt6} framework with the additional use of class \textit{FigureCanvasQTAgg} from \textit{matplotlib} library. The core of the functionality is relying on an abstraction, where the models are represented as a class with particular members - functions \textit{load} and \textit{predict} and already mentioned member variable of custom type \textit{Transformer}. There are also few more classes which make it easier to work with the prediction grid and the graphing canvas. Adding a support for more, completely different models is only a matter few lines of code as long s the abstraction is followed. Currently only NN, SVR and GP are supported.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.85 \textwidth]{figures/graph_tool2}
	\caption{A screenshot from the graphing tool with graph of NN model for $\alpha = 0\degree$.}
	\label{fig:graph-tool2}
\end{figure}


After choosing a model, the user can choose, which axis he wants to \textit{fix} and at which value. Before, we were always fixing the intensity to three values and sampled $L$ and $\alpha$. Then, the user chooses the ranges (\textit{min}, \textit{max}) of the remaining two axes alongside with the density of the sample (\textit{count}). Prediction of the same configuration as before but for NN model is shown in the figure \ref{fig:graph-tool2}. In both examples, notice the smooth transition between the intensities. Remember that there are almost no training point off the three intensities, so the predicted temperatures could not be very accurate.

Drawing of the error is currently supported only for the GP model. By clicking the \textit{Draw standard error} button the prediction standard error is drawn. The screenshot with standard error can be seen in figure \ref{fig:graph-tool3}.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.85 \textwidth]{figures/graph_tool3}
	\caption{A screenshot from the graphing tool with graph of standard error of GP model for $\alpha = 0\degree$.}
	\label{fig:graph-tool3}
\end{figure}

After previous discussion, it shouldn't surprise us that the uncertainty is larger for intensities that are not a power of 10. Similar error pattern is visible when $L$ is the fixed axis, instead of $\alpha$.

\section{Strategy for choosing the next simulations}


\section{Comparison to contemporary data}
\label{ch:comparison}
In this last section, the models are compared to a few of the previous works which studied the scaling of hot electron temperature.

First, the comparison will be done with respect to work of Cui et al. \cite{cui2013}, where they model scaling of hot electron temperature with intensity using:
\begin{equation}
	content...
\end{equation}


